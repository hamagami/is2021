{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "boosting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMs21rH5+Mk1lNj9qFt3vnN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamagami/is2021/blob/main/boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Various ensemble learning, Boosting, Random Forest, GradientBoosting, XGBoost"
      ],
      "metadata": {
        "id": "ADDegHqJB0zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will compare several discrimination algorithms　including some ensemble learnings. All can be run with the scikit-learn module. It's great."
      ],
      "metadata": {
        "id": "JYn7CecbD_QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd #pandas dataframe\n",
        "from sklearn.datasets import load_breast_cancer #dataset\n",
        "from sklearn.model_selection import train_test_split # Library to split data into training  and validation \n",
        "from sklearn import model_selection"
      ],
      "metadata": {
        "id": "Y9MoiiCoCLHW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression   #Logistic regression\n",
        "from sklearn.tree import DecisionTreeClassifier       #Decision Tree\n",
        "from sklearn.ensemble import RandomForestClassifier   #Random Forest(ensemble)\n",
        "from sklearn.ensemble import AdaBoostClassifier       #AdaBoost(ensemble)\n",
        "from sklearn.ensemble import GradientBoostingClassifier #GradientBoosting(ensemble)\n",
        "from xgboost import XGBClassifier                     #XGBoost(ensemble)\n"
      ],
      "metadata": {
        "id": "XlbwHdkMDX7a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prepare datset"
      ],
      "metadata": {
        "id": "7mT3hqiNEn1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "# sample data\n",
        "loaded_data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(loaded_data.data, loaded_data.target, random_state=seed)\n",
        "kfold = model_selection.KFold(n_splits = 5) \n",
        "scores = {}"
      ],
      "metadata": {
        "id": "KkMbjeRNCdwx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression\n",
        "The simplest discriminant learner (learned in Part 2). It discrimates the classes linearly by regressively approximating the boundary between the two classes with a logistic function. Naturally, it cannot deal with nonlinear problems."
      ],
      "metadata": {
        "id": "B8hEP478E2O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression\n",
        "lr_clf = LogisticRegression(solver='lbfgs', max_iter=10000) \n",
        "lr_clf.fit(X_train, y_train)\n",
        "results = model_selection.cross_val_score(lr_clf, X_test, y_test, cv = kfold) \n",
        "scores[('1.Logistic_regression', 'train_score')] = results.mean() \n",
        "scores[('1.Logistic_regression', 'test_score')] = lr_clf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "Us5jE3XaCiDA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decison tree\n",
        "Decision trees were covered in the previous article, Part 6. You will discrimate the classes while separating the data using simple rules. This improves the performance on training data, but the problem is that the generalization performance is low (over fittin) and the accuracy on test data is not improved."
      ],
      "metadata": {
        "id": "CtqGNP9xFuss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision tree\n",
        "dtc_clf = DecisionTreeClassifier(random_state=seed) \n",
        "dtc_clf.fit(X_train, y_train)\n",
        "results = model_selection.cross_val_score(dtc_clf, X_test, y_test, cv = kfold) \n",
        "scores[('2.decision_tree', 'train_score')] = results.mean() \n",
        "scores[('2.decision_tree', 'test_score')] = dtc_clf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "xGeEvLoJCisI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest\n",
        "Random forests are ensemble learning with decision trees as the weak learner. The number of trees, their depth, and the number of attributes to be bootstrap sampled need to be carefully designed, but the algorithm tends to perform well on average."
      ],
      "metadata": {
        "id": "_kKr6FEQGZZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest\n",
        "rfc_clf = RandomForestClassifier(max_depth=5, random_state=seed) \n",
        "rfc_clf.fit(X_train, y_train)\n",
        "results = model_selection.cross_val_score(rfc_clf, X_test, y_test, cv = kfold) \n",
        "scores[('3.Random Forest', 'train_score')] = results.mean()\n",
        "scores[('3.Random Forest', 'test_score')] = rfc_clf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "DGgtid20Cm5u"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaBoost (Adaptive Boosting)\n",
        "Boosting is a method of improving performance by sequentially adding weak learners; Adaboost \"adaptively\" adds new learners to compensate for the weaknesses of the weak learners created so far. The feature of Adaboost is that it can achieve high performance with a small amount of computation."
      ],
      "metadata": {
        "id": "l4GsHpUPG6yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBoost\n",
        "adb_clf = AdaBoostClassifier(n_estimators=100, random_state=seed) \n",
        "adb_clf.fit(X_train, y_train)\n",
        "results = model_selection.cross_val_score(adb_clf, X_test, y_test, cv = kfold) \n",
        "scores[('4.AdaBoost', 'train_score')] = results.mean()\n",
        "scores[('4.AdaBoost', 'test_score')] = adb_clf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "c9rTmWEkCk9d"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GradientBoosting\n",
        "Like AdaBoost, Gradient Boosting continuously modifies and adds a new weak-learner to the ensemble of weak learners; while AdaBoost changes the weights of the data, Gradient Boosting modifies the predictors to fit the residual errors."
      ],
      "metadata": {
        "id": "PuK9ndBWHgRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GradientBoosting (GBM)\n",
        "gbm_clf = GradientBoostingClassifier(random_state=seed)\n",
        "gbm_clf.fit(X_train, y_train)\n",
        "results = model_selection.cross_val_score(gbm_clf, X_test, y_test, cv = kfold) \n",
        "scores[('5.GBM', 'train_score')] = results.mean()\n",
        "scores[('5.GBM', 'test_score')] = gbm_clf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "wSbClVjKCqBn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost\n",
        "XGboost stands for \"eXtreme Gradient Boosting\". XGboost is a combination of gradient boosting and decision trees, and has very high generalization capability, and is often used in the top level programs in Kaggle."
      ],
      "metadata": {
        "id": "i7ptsaGYHjfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xgboost\n",
        "xgb_clf = XGBClassifier() \n",
        "xgb_clf.fit(X_train, y_train)\n",
        "results = model_selection.cross_val_score(xgb_clf, X_test, y_test, cv = kfold) \n",
        "scores[('6.xgboost', 'train_score')] = results.mean()\n",
        "scores[('6.xgboost', 'test_score')] = xgb_clf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "sMJeYTJMCtae"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of all algorithms."
      ],
      "metadata": {
        "id": "DnLSZ_r_IbN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル評価\n",
        "pd.Series(scores).unstack()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "Q7pQ3-yLCvqd",
        "outputId": "2a295d8b-d45b-49f8-ebf3-da3d1846dcef"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_score</th>\n",
              "      <th>train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.Logistic_regression</th>\n",
              "      <td>0.951049</td>\n",
              "      <td>0.936946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.decision_tree</th>\n",
              "      <td>0.881119</td>\n",
              "      <td>0.916010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.Random Forest</th>\n",
              "      <td>0.972028</td>\n",
              "      <td>0.944335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.AdaBoost</th>\n",
              "      <td>0.986014</td>\n",
              "      <td>0.943842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.GBM</th>\n",
              "      <td>0.965035</td>\n",
              "      <td>0.936946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6.xgboost</th>\n",
              "      <td>0.979021</td>\n",
              "      <td>0.930049</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       test_score  train_score\n",
              "1.Logistic_regression    0.951049     0.936946\n",
              "2.decision_tree          0.881119     0.916010\n",
              "3.Random Forest          0.972028     0.944335\n",
              "4.AdaBoost               0.986014     0.943842\n",
              "5.GBM                    0.965035     0.936946\n",
              "6.xgboost                0.979021     0.930049"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uoR9xienCzOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}